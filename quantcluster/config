#############################################
## AWS Credentials and Connection Settings ##
#############################################
[aws info]
AWS_ACCESS_KEY_ID=AKIAIEPSNPHUHQOY2OBQ
AWS_SECRET_ACCESS_KEY=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
AWS_USER_ID= 1583-3120-8712
AWS_REGION_NAME = us-east-1a
EC2_CERT = /mnt/botat/botat/starcluster/cert-XUV2SPPGNAXGVQJHSH73N3HDHQDQIXRN.pem
EC2_PRIVATE_KEY = /mnt/botat/botat/starcluster/pk-XUV2SPPGNAXGVQJHSH73N3HDHQDQIXRN.pem
# Uncomment these settings to use a proxy host when connecting to AWS
#AWS_PROXY = your.proxyhost.com
#AWS_PROXY_PORT = 8080
#AWS_PROXY_USER = yourproxyuser
#AWS_PROXY_PASS = yourproxypass
[key ec2]
KEY_LOCATION=~/.ssh/ec2.rsa
[global]
DEFAULT_TEMPLATE=default
# enable experimental features for this release
#ENABLE_EXPERIMENTAL=True
# number of seconds to wait when polling instances
REFRESH_INTERVAL=10
# specify a web browser to launch when viewing spot history plots
WEB_BROWSER=firefox
# split the config into multiple files
#INCLUDE=~/.starcluster/aws, ~/.starcluster/keys, ~/.starcluster/vols

###########################################
## Defining Additional Cluster Templates ##
###########################################
# You can also define multiple cluster templates. You can either supply all
# configuration options or create an # EXTENDS=<cluster_name> variable in
# the new cluster section to use all settings from <cluster_name> as defaults.

[cluster default]
KEYNAME = ec2
CLUSTER_SIZE = 1
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash 
NODE_IMAGE_ID = ami-3393a45a
NODE_INSTANCE_TYPE = m3.xlarge
PLUGINS = ipcluster, hadoop, hive

[cluster default_lts]
EXTENDS = default
NODE_IMAGE_ID = ami-765b3e1f

[cluster dual_eights]
KEYNAME = ec2
CLUSTER_SIZE = 2
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-52a0c53b
NODE_INSTANCE_TYPE = cc2.8xlarge
plugins = ipcluster, RAID_10
#VOLUMES = vol1, vol2, r1, r2, r3, r4, r5, r6, r7, r8

[cluster gpu_cluster]
KEYNAME = ec2
CLUSTER_SIZE = 2
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-4870eb21
NODE_INSTANCE_TYPE = cg1.4xlarge
VOLUMES = vol1
plugins = ipcluster

[cluster bigdata]
KEYNAME = ec2
CLUSTER_SIZE = 80
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-765b3e1f
NODE_INSTANCE_TYPE = m1.xlarge
PLUGINS = hadoop

[cluster hybrid]
KEYNAME = ec2
CLUSTER_SIZE = 3
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
MASTER_INSTANCE_TYPE = cc2.8xlarge
MASTER_IMAGE_ID = ami-52a0c53b
NODE_INSTANCE_TYPE = c1.medium
NODE_IMAGE_ID = ami-765b3e1f
plugins = ipcluster, hadoop, hive, hbase
VOLUMES = vol1

# SSD storage(2x1TB)
# [cluster ssd]
# CLUSTER_SIZE = 1
# NODE_INSTANCE_TYPE = hi1.4xlarge
# NODE_IMAGE_ID = ami-52a0c53b

[cluster webapp]
KEYNAME = ec2
CLUSTER_SIZE = 1
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-765b3e1f
NODE_INSTANCE_TYPE = m1.xlarge
PLUGINS = hadoop, hive
VOLUMES = 


#############################
## Configuring EBS Volumes ##
#############################
# StarCluster can attach one or more EBS volumes to the master and then
# NFS_share these volumes to all of the worker nodes. A new [volume] section
# must be created for each EBS volume you wish to use with StarCluser. The
# section name is a tag for your volume. This tag is used in the VOLUMES
# setting of a cluster template to declare that an EBS volume is to be mounted
# and nfs shared on the cluster. (see the commented VOLUMES setting in the
# example 'smallcluster' template above) Below are some examples of defining
# and configuring EBS volumes to be used with StarCluster:

# By default StarCluster will attempt first to mount the entire volume device,
# failing that it will try the first partition. If you have more than one
# partition you will need to set the PARTITION number, e.g.:
# [volume oceandata]
# VOLUME_ID = vol-d7777777
# MOUNT_PATH = /mydata
# PARTITION = 2

# Uploading and creating EBS volumes can be done with:
# >> starcluster createvolume --name=vol1 1024 us-east-1a

# 760 GB of TAQ data (1 TB drive)
[volume vol1]
VOLUME_ID = vol-67a21314
MOUNT_PATH = /vols/vol1

# Options data (1 TB drive)
[volume vol2]
VOLUME_ID = vol-78c7983e
MOUNT_PATH = /vols/vol2

# Options data (1 TB drive)
[volume vol3]
VOLUME_ID = vol-0afbdf50
MOUNT_PATH = /vols/vol3


############################################
## Configuring Security Group Permissions ##
############################################
# Sections starting with "permission" define security group rules to
# automatically apply to newly created clusters. PROTOCOL in the following
# examples can be can be: tcp, udp, or icmp. CIDR_IP defaults to 0.0.0.0/0 or
# "open to the # world"

# open port 80 on the cluster to the world
[permission http]
PROTOCOL = tcp
FROM_PORT = 80
TO_PORT = 80

# open https on the cluster to the world
[permission https]
PROTOCOL = tcp
FROM_PORT = 443
TO_PORT = 443

# open port 80 on the cluster to an ip range using CIDR_IP
# [permission http]
# PROTOCOL = tcp
# FROM_PORT = 80
# TO_PORT = 80
# CIDR_IP = 18.0.0.0/8

# restrict ssh access to a single ip address (<your_ip>)
# [permission ssh]
# PROTOCOL = tcp
# FROM_PORT = 22
# TO_PORT = 22
# CIDR_IP = <your_ip>/32


#####################################
## Configuring StarCluster Plugins ##
#####################################
# Sections starting with "plugin" define a custom python class which perform
# additional configurations to StarCluster's default routines. These plugins
# can be assigned to a cluster template to customize the setup procedure when
# starting a cluster from this template (see the commented PLUGINS setting in
# the 'smallcluster' template above). Below is an example of defining a user
# plugin called 'myplugin':

# Note for bigdata:
# hadoop_new: uses S3 cladogenesis_downloads bucket to get the Hadoop JAR.
#
# The number of Hadoop mappers and reducers should be set to 4
# when using the m1.xlarge instances. They are quad core and this
# gives only around 40% disk utilization.
#
# Also, need to add this to bashrc:
#      export PATH=/opt/hadoop/bin:/opt/hive/bin:/opt/hbase/bin:$PATH

[plugin RAID_10]
SETUP_CLASS = RAID_10.RAID_10

[plugin hadoop]
SETUP_CLASS = Hadoop.Hadoop

# Configure a hadoop cluster. (includes dumbo setup)
#[plugin hadoop]
#SETUP_CLASS = starcluster.plugins.hadoop.Hadoop

[plugin hive]
SETUP_CLASS = Hive.Hive

[plugin hbase]
SETUP_CLASS = Hbase.Hbase

######################
## Built-in Plugins ##
######################
# The following plugins ship with StarCluster and should work out-of-the-box.
# Uncomment as needed. Don't forget to update your PLUGINS list!
# See http://web.mit.edu/star/cluster/docs/latest/plugins for plugin details.
#
# Use this plugin to install one or more packages on all nodes using apt-get
[plugin pkginstaller]
SETUP_CLASS = starcluster.plugins.pkginstaller.PackageInstaller
PACKAGES =
#sysstat
#mongodb, python-pymongo

# Use this plugin to create one or more cluster users and download all user ssh
# keys to $HOME/.starcluster/user_keys/<cluster>-<region>.tar.gz
[plugin createusers]
SETUP_CLASS = starcluster.plugins.users.CreateUsers
#NUM_USERS = 30
# you can also comment out NUM_USERS and specify exact usernames, e.g.
usernames = gary, chris, jimmie
DOWNLOAD_KEYS = True

# Use this plugin to configure the Condor queueing system
# [plugin condor]
# SETUP_CLASS = starcluster.plugins.condor.CondorPlugin
#
# The SGE plugin is enabled by default and not strictly required. Only use this
# if you want to tweak advanced settings in which case you should also set
# DISABLE_QUEUE=TRUE in your cluster template. See the plugin doc for more
# details.
# [plugin sge]
# SETUP_CLASS = starcluster.plugins.sge.SGEPlugin
# MASTER_IS_EXEC_HOST = False
#
# The IPCluster plugin configures a parallel IPython cluster with optional
# web notebook support. This allows you to run Python code in parallel with low
# latency message passing via ZeroMQ.
[plugin ipcluster]
SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster
ENABLE_NOTEBOOK = True
NOTEBOOK_PASSWD = ruston235711!
NOTEBOOK_DIRECTORY = /home/sgeadmin

# <MANUAL USE ONLY>
[plugin ipclusterStop]
SETUP_CLASS = starcluster.plugins.ipcluster.IPClusterStop

# <MANUAL USE ONLY>
[plugin ipclusterRestart]
SETUP_CLASS = starcluster.plugins.ipcluster.IPClusterRestartEngines


# Use this plugin to create a cluster SSH "dashboard" using tmux. The plugin
# creates a tmux session on the master node that automatically connects to all
# the worker nodes over SSH. Attaching to the session shows a separate window
# for each node and each window is logged into the node via SSH.
[plugin tmux]
SETUP_CLASS = starcluster.plugins.tmux.TmuxControlCenter
#
# Use this plugin to change the default MPI implementation on the
# cluster from OpenMPI to MPICH2.
# [plugin mpich2]
# SETUP_CLASS = starcluster.plugins.mpich2.MPICH2Setup
#

# Configure a distributed MySQL Cluster
[plugin mysqlcluster]
#SETUP_CLASS = starcluster.plugins.mysql.MysqlCluster
SETUP_CLASS = mysql.MysqlCluster
NUM_REPLICAS = 2
DATA_MEMORY = 1G
INDEX_MEMORY = 18M
DUMP_DIR = /mnt/
DUMP_FILE = test.sql
DUMP_INTERVAL = 60
DEDICATED_QUERY = True
NUM_DATA_NODES = 2
# !!! ERROR - command 'DEBIAN_FRONTEND='noninteractive' apt-get -o Dpkg::Options::='--force-confnew' -y --force-yes install mysql-cluster-server' failed with status 100

# [plugin mysql]
# SETUP_CLASS = mysql.MysqlCluster

# Install and setup an Xvfb server on each cluster node
# [plugin xvfb]
# SETUP_CLASS = starcluster.plugins.xvfb.XvfbSetup
